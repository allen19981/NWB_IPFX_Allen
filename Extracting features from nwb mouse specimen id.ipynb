{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IPFX libraries\n",
    "from ipfx.dataset.create import create_ephys_data_set\n",
    "from ipfx.data_set_features import extract_data_set_features\n",
    "from ipfx.utilities import drop_failed_sweeps\n",
    "\n",
    "# Load pandas library too\n",
    "import pandas as pd\n",
    "mouse_data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipfx.data_access import get_archive_info\n",
    "archive_url, file_manifest, experiment_metadata = get_archive_info(dataset=\"mouse\")\n",
    "mouse_specimen_id = list(experiment_metadata['cell_specimen_id'])\n",
    "file_manifest\n",
    "file_name_dict = {}\n",
    "for index, row in file_manifest.iterrows():\n",
    "    if row[\"file_name\"][-4:] == \".nwb\":\n",
    "        file_name_dict[row[\"cell_specimen_id\"]] = row[\"file_name\"]\n",
    "        \n",
    "new_file_name = {}\n",
    "index = 0\n",
    "for item in file_name_dict:\n",
    "    if index > 1158:\n",
    "        new_file_name[item] = file_name_dict[item]\n",
    "    index += 1\n",
    "\n",
    "file_name_dict = new_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cell_ephys_features(lsa_results):\n",
    "    hero_keep_features = ['adapt', 'avg_rate', 'first_isi', 'isi_cv', 'latency', 'mean_isi', 'median_isi', \n",
    "                          'stim_amp']\n",
    "    rheo_keep_features = ['threshold_v', 'peak_v', 'trough_v', \n",
    "     'fast_trough_v', 'adp_v', 'width', 'upstroke_downstroke_ratio', 'peak_t', 'fast_trough_t', 'trough_t']\n",
    "    \n",
    "    overall_cell_keep_features = ['v_baseline', 'rheobase_i', 'fi_fit_slope', \n",
    "                                  'sag', 'vm_for_sag', 'input_resistance', 'tau']\n",
    "    \n",
    "    hero_small_dict = lsa_results['hero_sweep'][hero_keep_features]\n",
    "    rheobase_sweep_index = lsa_results['rheobase_sweep'].name\n",
    "    rheobase_sweep = lsa_results['spikes_set'][rheobase_sweep_index].iloc[0]\n",
    "    \n",
    "    rheo_spike_small_dict = rheobase_sweep[rheo_keep_features]\n",
    "    rheo_first_isi = lsa_results['rheobase_sweep']['first_isi']\n",
    "    rheo_spike_small_dict['rheo_first_isi'] = rheo_first_isi\n",
    "\n",
    "    spike_comb_dict = {**hero_small_dict, **rheo_spike_small_dict}\n",
    "\n",
    "    overall_cell_features = {x: lsa_results[x] for x in overall_cell_keep_features if x in lsa_results}\n",
    "    final_cell_feature_dict = {**spike_comb_dict, **overall_cell_features}\n",
    "    return(final_cell_feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from ipfx.dataset.create import create_ephys_data_set\n",
    "from ipfx.utilities import drop_failed_sweeps\n",
    "\n",
    "for item in file_name_dict:\n",
    "    specimen_id = item\n",
    "    file_name_spec = file_name_dict[item]\n",
    "#     row = file_manifest.query('cell_specimen_id == {} & technique == \"intracellular_electrophysiology\"'.format(specimen_id))\n",
    "#     archive_uri = row['archive_uri'].values[0]\n",
    "#     file_name = row['file_name'].values[0]\n",
    "#     print('Intrinsic ephys file url: ' + archive_uri)\n",
    "#     print('Intrinsic ephys file name: ' + file_name)\n",
    "\n",
    "#     print('downloading data for cell from DANDI...')\n",
    "    try:\n",
    "#         urllib.request.urlretrieve(archive_uri, file_name)\n",
    "#         print('data downloaded!')\n",
    "\n",
    "        # Create Ephys Data Set\n",
    "\n",
    "        print('loading dataset into data structure...')\n",
    "        nwb_file_name = '/external/rprshnas01/netdata_kcni/stlab/AIBS_patchseq_2020/mouse/ephys/000020/{}'.format(file_name_spec)\n",
    "        data_set = create_ephys_data_set(nwb_file=nwb_file_name) ##loads nwb file into ipfx data structure\n",
    "\n",
    "        # Drop failed sweeps: sweeps with incomplete recording or failing QC criteria\n",
    "        drop_failed_sweeps(data_set)\n",
    "\n",
    "        from ipfx.feature_extractor import SpikeFeatureExtractor, SpikeTrainFeatureExtractor\n",
    "\n",
    "        import ipfx.stimulus_protocol_analysis as spa\n",
    "        from ipfx.epochs import get_stim_epoch\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "        # get sweep table of Long Square sweeps\n",
    "        long_square_table = data_set.filtered_sweep_table(\n",
    "            stimuli=data_set.ontology.long_square_names\n",
    "        )\n",
    "        long_square_sweeps = data_set.sweep_set(long_square_table.sweep_number)\n",
    "\n",
    "        # Select epoch corresponding to the actual recording from the sweeps\n",
    "        # and align sweeps so that the experiment would start at the same time\n",
    "        long_square_sweeps.select_epoch(\"recording\")\n",
    "        long_square_sweeps.align_to_start_of_epoch(\"experiment\")\n",
    "\n",
    "        # find the start and end time of the stimulus\n",
    "        # (treating the first sweep as representative)\n",
    "        stim_start_index, stim_end_index = get_stim_epoch(long_square_sweeps.i[0])\n",
    "        stim_start_time = long_square_sweeps.t[0][stim_start_index]\n",
    "        stim_end_time = long_square_sweeps.t[0][stim_end_index]\n",
    "\n",
    "        # build the extractors\n",
    "        spfx = SpikeFeatureExtractor(start=stim_start_time, end=stim_end_time)\n",
    "        sptfx = SpikeTrainFeatureExtractor(start=stim_start_time, end=stim_end_time)\n",
    "\n",
    "        # run the analysis and print out a few of the features\n",
    "        long_square_analysis = spa.LongSquareAnalysis(spfx, sptfx, subthresh_min_amp=-100.0)\n",
    "        data = long_square_analysis.analyze(long_square_sweeps)\n",
    "\n",
    "        specimen_dict = summarize_cell_ephys_features(data)\n",
    "        mouse_data_dict[specimen_id] = specimen_dict\n",
    "        \n",
    "        print(\"{} extracted\".format(item))\n",
    "        dfObj = pd.DataFrame(mouse_data_dict)\n",
    "        dfObj.to_csv('mouse_features_from_nwb6.csv')\n",
    "        \n",
    "    except urllib.error.HTTPError as err:\n",
    "        print(err.code)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
